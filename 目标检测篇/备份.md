

6. 你知道哪些边缘端部署的方案？
   目前大多数深度学习算法模型要落地对算力要求还是比较高的，如果在服务器上，可以使用GPU进行加速，但是在边缘端或者算力匮乏的开发板子上，不得不对模型进一步的压缩或者改进，也可以针对特定的场景使用市面上现有的推理优化加速框架进行推理。目前来说比较常见的几种部署方案为：
   nvidia GPU：pytorch->onnx->TensorRT
   intel CPU： pytorch->onnx->openvino
   移动端（手机、开发板等）：pytorch->onnx->MNN、NCNN、TNN、TF-lite、Paddle-lite、RKNN等
7. 你最常用的几种目标检测算法是什么？为什么选择这些算法，你选择它们的场景分别是什么？
   在工作中，我通常会根据不同的任务选取不同的算法模型：
   目标检测：YOLOv5、YOLOv3、CenterNet、SSD、Faster RCNN、EfficientDet；
   图像分类：mobileNetv2、mobileNetv3、ghostNet、ResNet系列、ShuffleNetV2、EfficientNet；
   实例分割：mask-rcnn、yolact、solo；
   语义分割：deeplabv3、deeplabv3+、UNet；
   文本检测：CTPN、PSENet、DBNet、YOLOV5；
   文本识别：CRNN+CTC、CRNN+Attention；
   通常，我比较喜欢性能好的模型，性能的指标由两部分，一个是精度，一个是速度。比如在目标检测中，用的比较多的是YOLO系列，特别是v4、v5出来后。通常在图像分类的任务上，分类并不困难的情况下会选择一些轻量型的网络，能够一定程度上节省算力资源。其他领域的任务算法抉择也大同小异。



12. 你还了解当下哪些比较流行的目标检测算法？
    目前比较流行的目标检测算法有以下几种类型，不局限于这几种：
    anchor-based：YOLOv3、YOLOv4、YOLOv5、pp-YOLO、SSD、Faster-R-CNN、Cascade R-CNN、EfficientDet，RetinaNet、MTCNN；
    anchor-free：CornerNet、CenterNet、CornerNet-lite、FCOS；
    transform：DETR；
    mobile-detector：mobileNet-YOLO、mobileNet-SSD、tiny-YOLO、nanodet、YOLO-fastest、YOLObile、mobilenet-retinaNet、MTCNN；
    还有很多很多。。。mmdetection里面就实现了几十种，可以去看一看，这里面最想总结的是移动端的det，很多都是一些大佬在原生算法基础上的改进，有时间出一篇文章专门记录这个类型的检测器。
13. EfficentDet为什么可以做到速度兼精度并存 ？
14. 介绍Faster R-CNN和Cascade R-CNN

Faster-RCNN是基于候选区域的双阶段检测器代表作，总的来说可以分为四部分：首先是主干卷积网络的特征提取，然后是RPN层，RPN层通过softmax判断anchors属于positive或者negative，再利用边框回归修正anchors获得精确的候选区域，RPN生成了大量的候选区域，这些候选区域和feature maps一起送入ROI pooling中，得到了候选特征区域，最后送入分类层中进行类别判断和边框回归，得到最终的预测结果。

Cascade R-CNN算法是在Faster R-CNN上的改进，通过级联几个检测网络达到不断优化预测结果的目的，预普通的级联不同，Cascade R-CNN的几个检测网络是基于不同的IoU阈值确定的正负样本上训练得到的。简单来说cascade R-CNN是由一系列的检测模型组成，每个检测模型都基于不同IoU阈值的正负样本训练得到，前一个检测模型的输出作为后一个检测模型的输入，因此是stage by stage的训练方式，而且越往后的检测模型，其界定正负样本的IoU阈值是不断上升的。

15. SSD相比于YOLO做了哪些改进？

这里说的是SSD相对于YOLOv1的改进，因为现在SSD已经不更了，但是YOLO还如日中天，已经发展到v5，性能在目标检测算法里一骑绝尘。那么最原始的SSD相对于YOLOv1做了哪些改进呢？

SSD提取了不同尺度的特征图来做检测，而YOLO在检测是只用了最高层的Feature maps；
SSD引入了Faster-RCNN的anchor机制，采用了不同尺度和长宽比的先验框；
SSD网络结构是全卷积，采用卷积做检测，YOLO用到了FC（全连接）层；
16. 介绍SSD原理

SSD算法流程：
输入一幅图，让图片经过卷积神经网络（VGG）提取特征，生成feature map
抽取其中六层的feature map，然后分别在这些feature map层上面的每一个点构造4、个不同尺度大小的default box，然后分别进行检测和分类（各层的个数不同，但每个点都有）
将生成的所有default box都集合起来，全部丢到NMS中，输出筛选后的default box。

17. 了解哪些开源的移动端轻量型目标检测？

轻量型的目标检测其实有很多，大多数都是基于YOLO、SSD的改进，当然也有基于其他算法改的；比较常用的改进方法是使用轻量型的backbone替换原始的主干网络，例如mobilenet-ssd、mobilenet-YOLOv3、YOLO-fastest、YOLObile、YOLO-nano、nanodet、tiny-YOLO等等，在减少了计算量的同时保持着不错的精度，经过移动部署框架推理后，无论是在服务器还是移动端都有着不错的精度和速度。

18. 对于小目标检测，你有什么好的方案或者技巧？

图像金字塔和多尺度滑动窗口检测（MTCNN）
多尺度特征融合检测（FPN、PAN、ASFF等）
增大训练、检测图像分辨率；
超分策略放大后检测；
19. 介绍一下NMS和IoU的原理；

NMS全称是非极大值抑制，顾名思义就是抑制不是极大值的元素。在目标检测任务中，通常在解析模型输出的预测框时，预测目标框会非常的多，其中有很多重复的框定位到了同一个目标，NMS的作用就是用来除去这些重复框，从而获得真正的目标框。而NMS的过程则用到了IoU，IoU是一种用于衡量真实和预测之间相关度的标准，相关度越高，该值就越高。IoU的计算是两个区域重叠的部分除以两个区域的集合部分，简单的来说就是交集除以并集。

在NMS中，首先对预测框的置信度进行排序，依次取置信度最大的预测框与后面的框进行IoU比较，当IoU大于某个阈值时，可以认为两个预测框框到了同一个目标，而置信度较低的那个将会被剔除，依次进行比较，最终得到所有的预测框。

20. 目标检测单阶段和双阶段优缺点，双阶段的为什么比单阶段的效果要好？
21. 目标检测中如何处理正负样本不平衡的问题？
22. YOLOv3为什么这么快？

YOLOv3和SSD比网络更加深了，虽然anchors比SSD少了许多，但是加深的网络深度明显会增加更多的计算量，那么为什么YOLOv3会比SSD快3倍？
SSD用的很老的VGG16，V3用的其最新原创的Darknet，darknet-53与resnet的网络结构，darknet-53会先用1x1的卷积核对feature降维，随后再利用3x3的卷积核升维，这个过程中，就会大大降低参数的计算量以及模型的大小，有点类似于低秩分解。究其原因是做了很多优化，比如用卷积替代替代全连接，1X1卷积减小计算量等。

23. 你认为当前目标检测算法发展的趋势是什么？现阶段存在什么难点？
24. 你知道哪些模型压缩和推理优化的方案？
25. 模型部署
26. 介绍一下YOLOx，它相对于YOLOv3、YOLOv5做了哪些改进？这样改进取得了什么样的效果和好处？你怎么评价这篇论文？
27. 为了适配边缘部署要求，AI算法怎么做适配？
28. 在模型效果和效率之间怎么做平衡和取舍？怎么在不牺牲效果的前提下提高效率？
29. 算法上线后怎么持续做迭代？
30. 基于Anchor-base的目标检测算法相对于基于Anchor-free的目标检测算法有什么缺陷？

基于Anchor-base的目标检测算法需要在训练前通过聚类决定一系列的anchor，这些anchor具有很强的领域性，泛化差；
anchor机制增加了detection heads的复杂性，增加了预测数量，这在边缘AI系统中，是一个瓶颈。
anchor-free在YOLOx中的做法是：

在每个位置只预测一次，预测四个值：左上角xy坐标的偏移、宽、高。

将中心3X3=9的区域设置为正样本，称作：center sampling。

31. 解释一下YOLOx解藕头的设计思想

classification和 regression 是有冲突的，但是YOLO系列工作还是将两者放在一个head里面。

32. 解释YOLOx中SimOTA标签自动分配策略的思想
33. Transformer落地难点有哪些？
34. TensorRT的优化原理

算子融合（网络层合并）：简单来说就是通过融合一些计算op或者去掉一些多余op来减少数据流通次数以及显存的频繁使用来提速，融合之后的计算图的层次更少了，占用的CUDA核心数也少了，因此整个模型结构会更小，更快，更高效
低精度量化：量化即IN8量化或者FP16以及TF32等不同于常规FP32精度的使用，这些精度可以显著提升模型执行速度并且不会保持原先模型的精度，更低的数据精度将会使得内存占用和延迟更低，模型体积更小
cuda核自动调整：根据不同的显卡构架、SM数量、内核频率等，选择不同的优化策略以及计算方式，寻找最合适当前构架的计算方式，不同的硬件TensorRT 都会做对应的优化，得到优化后的engine
动态张量显存：在每个tensor的使用期间，TensorRT会为其指定显存，避免显存重复申请，减少内存占用和提高重复使用效率
多流执行：使用CUDA中的stream技术，最大化实现并行操作

